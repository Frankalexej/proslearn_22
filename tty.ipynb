{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from librosa.util import index_to_slice\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# meta data file (with syllable information)\n",
    "from PretermDataLoader import DataLoader as dl\n",
    "import audioPreprocess as ap\n",
    "import random\n",
    "import dataPreprocess as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafile = 'guide_test_syllableInfor.pkl'\n",
    "datanum = 1000 # assume that you want 10000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dl()\n",
    "meta = loader.get_metadata(metafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mels,indexes = loader.load_data('mel',datanum)\n",
    "spectrograms = loader.load_data('spectrogram',datanum,indexes)[0]\n",
    "soundpaths = [meta['filepath'][i] for i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 126)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mels: [datanum, (mel_dim, length)]\n",
    "# 長度固定，因此可以直接轉成numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franklhtan/anaconda3/envs/pros22/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# or you can get a bunch of indexlists that keeps the original distribution first\n",
    "meta['index'] = meta.index\n",
    "indexset = meta[['stress_type','index','suid']]\n",
    "# Split into subsets\n",
    "subsets_index = dp.split_into_subsets(indexset, num_subsets=100, sortkey='stress_type')\n",
    "# #Display the resulting subsets\n",
    "# for i, subset in enumerate(subsets):\n",
    "#     dp.checkDistribution(subset['stress_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stress_type</th>\n",
       "      <th>index</th>\n",
       "      <th>suid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98594</th>\n",
       "      <td>1</td>\n",
       "      <td>98594</td>\n",
       "      <td>446-123502-0022-0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22107</th>\n",
       "      <td>0</td>\n",
       "      <td>22107</td>\n",
       "      <td>1502-122619-0078-0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27266</th>\n",
       "      <td>1</td>\n",
       "      <td>27266</td>\n",
       "      <td>2092-145706-0037-0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69203</th>\n",
       "      <td>0</td>\n",
       "      <td>69203</td>\n",
       "      <td>3240-131232-0006-0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98418</th>\n",
       "      <td>0</td>\n",
       "      <td>98418</td>\n",
       "      <td>446-123502-0017-0045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109917</th>\n",
       "      <td>1</td>\n",
       "      <td>109917</td>\n",
       "      <td>5163-39921-0022-0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125275</th>\n",
       "      <td>1</td>\n",
       "      <td>125275</td>\n",
       "      <td>78-368-0045-0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120438</th>\n",
       "      <td>1</td>\n",
       "      <td>120438</td>\n",
       "      <td>7402-59171-0047-0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80574</th>\n",
       "      <td>0</td>\n",
       "      <td>80574</td>\n",
       "      <td>3807-4955-0009-0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20314</th>\n",
       "      <td>1</td>\n",
       "      <td>20314</td>\n",
       "      <td>1502-122619-0033-0033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1392 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stress_type   index                   suid\n",
       "98594            1   98594   446-123502-0022-0002\n",
       "22107            0   22107  1502-122619-0078-0047\n",
       "27266            1   27266  2092-145706-0037-0037\n",
       "69203            0   69203  3240-131232-0006-0042\n",
       "98418            0   98418   446-123502-0017-0045\n",
       "...            ...     ...                    ...\n",
       "109917           1  109917   5163-39921-0022-0007\n",
       "125275           1  125275       78-368-0045-0016\n",
       "120438           1  120438   7402-59171-0047-0042\n",
       "80574            0   80574    3807-4955-0009-0038\n",
       "20314            1   20314  1502-122619-0033-0033\n",
       "\n",
       "[1392 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets_index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it seems certain that the loading is not on-the-full. \n",
    "Therefore, we can happily design loading functions based on this mechanism. We don't have to design a \"load-for-all\" mechanism, because, anyway, the time consumption and memory usage is not \"the full\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "Design how we pass in data for different dataloaders and how we plan learning curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be easy to do just following our old practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, Counter\n",
    "\n",
    "class LearningPathPlanner:\n",
    "    \"\"\"\n",
    "    A planner for generating a learning path from a pool of dataset IDs, allowing for control over \n",
    "    probability of selecting new or old datasets, and imposing revisit limits on old datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_ids, total_epochs, p1=0.5, revisit_limit=5):\n",
    "        \"\"\"\n",
    "        Initializes the LearningPathPlanner.\n",
    "\n",
    "        Args:\n",
    "            dataset_ids (list): List of dataset IDs representing the pool of available datasets.\n",
    "            total_epochs (int): The total number of epochs for which to generate a learning path.\n",
    "            p1 (float): Probability of selecting a new dataset in each epoch (between 0 and 1).\n",
    "            revisit_limit (int): Maximum number of times each dataset can be revisited before being removed.\n",
    "        \"\"\"\n",
    "        self.dataset_ids = dataset_ids  # Pool of dataset IDs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.p1 = p1\n",
    "        self.revisit_limit = revisit_limit\n",
    "        self.new_datasets = set(dataset_ids)  # Datasets not yet seen\n",
    "        self.old_datasets = deque()  # Queue to track datasets that have been used\n",
    "        self.revisit_count = Counter()  # Counter to track the number of revisits for each dataset\n",
    "\n",
    "    def get_next_dataset(self):\n",
    "        \"\"\"\n",
    "        Determines the next dataset ID to use based on the learning path logic.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the next dataset to use for training.\n",
    "        \"\"\"\n",
    "        # Decide whether to select a new or an old dataset based on probability p1\n",
    "        if self.new_datasets and random.random() < self.p1:\n",
    "            # Choose a new dataset\n",
    "            next_dataset = self.new_datasets.pop()\n",
    "            self.old_datasets.append(next_dataset)  # Move to old datasets\n",
    "            self.revisit_count[next_dataset] = 0  # Initialize revisit count\n",
    "\n",
    "        else:\n",
    "            # Choose an old dataset, if any are available\n",
    "            if self.old_datasets:\n",
    "                next_dataset = random.choice(self.old_datasets)\n",
    "                self.revisit_count[next_dataset] += 1\n",
    "\n",
    "                # Check if the revisit limit is reached\n",
    "                if self.revisit_count[next_dataset] >= self.revisit_limit:\n",
    "                    # Remove dataset if revisit limit is reached\n",
    "                    self.old_datasets.remove(next_dataset)\n",
    "                    del self.revisit_count[next_dataset]\n",
    "            else:\n",
    "                # Fallback to a new dataset if no old datasets are available\n",
    "                next_dataset = self.new_datasets.pop()\n",
    "                self.old_datasets.append(next_dataset)\n",
    "                self.revisit_count[next_dataset] = 0\n",
    "\n",
    "        return next_dataset\n",
    "\n",
    "    def generate_learning_path(self):\n",
    "        \"\"\"\n",
    "        Generates a full learning path for the specified total number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset IDs representing the planned learning path.\n",
    "        \"\"\"\n",
    "        learning_path = []\n",
    "        for _ in range(self.total_epochs):\n",
    "            next_dataset = self.get_next_dataset()\n",
    "            learning_path.append(next_dataset)\n",
    "        return learning_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned Learning Path: [1, 1, 2, 3, 1, 4, 1, 5, 6, 7, 8, 4, 9, 6, 3, 2, 5, 9, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "# Define a pool of dataset IDs\n",
    "dataset_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Initialize the planner with total epochs, probability p1, and revisit limit\n",
    "planner = LearningPathPlanner(dataset_ids, total_epochs=20, p1=0.5, revisit_limit=3)\n",
    "\n",
    "# Generate a learning path\n",
    "learning_path = planner.generate_learning_path()\n",
    "print(\"Planned Learning Path:\", learning_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import deque, Counter\n",
    "\n",
    "class LearningPathPlanner:\n",
    "    \"\"\"\n",
    "    A planner for generating a learning path from a pool of dataset IDs, where revisit probability exponentially decreases \n",
    "    with each visit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_ids, total_epochs, p1=0.5, decay_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the LearningPathPlanner.\n",
    "\n",
    "        Args:\n",
    "            dataset_ids (list): List of dataset IDs representing the pool of available datasets.\n",
    "            total_epochs (int): The total number of epochs for which to generate a learning path.\n",
    "            p1 (float): Probability of selecting a new dataset in each epoch (between 0 and 1).\n",
    "            decay_rate (float): The rate at which revisit probability decreases exponentially with each visit.\n",
    "        \"\"\"\n",
    "        self.dataset_ids = dataset_ids  # Pool of dataset IDs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.p1 = p1\n",
    "        self.decay_rate = decay_rate\n",
    "        self.new_datasets = set(dataset_ids)  # Datasets not yet seen\n",
    "        self.old_datasets = deque()  # Queue to track datasets that have been used\n",
    "        self.visit_count = Counter()  # Counter to track the number of visits for each dataset\n",
    "\n",
    "    def get_exponential_probability(self, visits):\n",
    "        \"\"\"\n",
    "        Calculates the probability weight for a dataset based on its visit count using exponential decay.\n",
    "\n",
    "        Args:\n",
    "            visits (int): The number of times the dataset has been visited.\n",
    "\n",
    "        Returns:\n",
    "            float: The probability weight for the dataset.\n",
    "        \"\"\"\n",
    "        return math.exp(-self.decay_rate * visits)\n",
    "\n",
    "    def get_next_dataset(self):\n",
    "        \"\"\"\n",
    "        Determines the next dataset ID to use based on the learning path logic, with exponential decay in revisit probability.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the next dataset to use for training.\n",
    "        \"\"\"\n",
    "        # Decide whether to select a new or an old dataset based on probability p1\n",
    "        if self.new_datasets and random.random() < self.p1:\n",
    "            # Choose a new dataset\n",
    "            next_dataset = self.new_datasets.pop()\n",
    "            self.old_datasets.append(next_dataset)  # Move to old datasets\n",
    "            self.visit_count[next_dataset] = 0  # Initialize visit count\n",
    "\n",
    "        else:\n",
    "            # Choose an old dataset with probability exponentially decreasing by visit count\n",
    "            if self.old_datasets:\n",
    "                # Calculate weights for old datasets based on exponential decay\n",
    "                weights = [self.get_exponential_probability(self.visit_count[ds]) for ds in self.old_datasets]\n",
    "                total_weight = sum(weights)\n",
    "                probabilities = [w / total_weight for w in weights]\n",
    "\n",
    "                # Randomly choose an old dataset based on calculated probabilities\n",
    "                next_dataset = random.choices(list(self.old_datasets), weights=probabilities, k=1)[0]\n",
    "                self.visit_count[next_dataset] += 1  # Increment visit count\n",
    "\n",
    "            else:\n",
    "                # Fallback to a new dataset if no old datasets are available\n",
    "                next_dataset = self.new_datasets.pop()\n",
    "                self.old_datasets.append(next_dataset)\n",
    "                self.visit_count[next_dataset] = 0\n",
    "\n",
    "        return next_dataset\n",
    "\n",
    "    def generate_learning_path(self):\n",
    "        \"\"\"\n",
    "        Generates a full learning path for the specified total number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset IDs representing the planned learning path.\n",
    "        \"\"\"\n",
    "        learning_path = []\n",
    "        for _ in range(self.total_epochs):\n",
    "            next_dataset = self.get_next_dataset()\n",
    "            learning_path.append(next_dataset)\n",
    "        return learning_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned Learning Path: ['D', 'D', 'D', 'F', 'E', 'C', 'B', 'I', 'J', 'J', 'E', 'K', 'G', 'H', 'H', 'B', 'I', 'K', 'A', 'K', 'A', 'G', 'A', 'C', 'J', 'H', 'I', 'H', 'I', 'E', 'G', 'C', 'G', 'D', 'F', 'C', 'K', 'H', 'B', 'C', 'I', 'K', 'B', 'G', 'D', 'A', 'D', 'C', 'H', 'J', 'E', 'A', 'K', 'E', 'G', 'J', 'K', 'C', 'E', 'F', 'F', 'D', 'E', 'D', 'I', 'B', 'B', 'F', 'D', 'G', 'J', 'A', 'E', 'K', 'F', 'B', 'E', 'F', 'A', 'J', 'B', 'J', 'A', 'J', 'F', 'I', 'D', 'H', 'H', 'G', 'C', 'F', 'B', 'H', 'B', 'B', 'I', 'D', 'K', 'C', 'D', 'E', 'J', 'H', 'A', 'F', 'E', 'I', 'A', 'K', 'K', 'G', 'I', 'F', 'B', 'F', 'F', 'A', 'J', 'H', 'C', 'J', 'H', 'C', 'B', 'H', 'I', 'I', 'B', 'J', 'G', 'G', 'D', 'I', 'H', 'H', 'K', 'B', 'C', 'A', 'H', 'G', 'D', 'D', 'H', 'K', 'E', 'E', 'G', 'F', 'A', 'C', 'F', 'E', 'G', 'I', 'G', 'A', 'F', 'E', 'C', 'K', 'A', 'A', 'J', 'B', 'B', 'J', 'E', 'K', 'K', 'D', 'A', 'A', 'J', 'E', 'H', 'C', 'B', 'A', 'I', 'E', 'I', 'K', 'F', 'E', 'H', 'E', 'C', 'B', 'K', 'C', 'A', 'G', 'J', 'F', 'J', 'I', 'I', 'F']\n"
     ]
    }
   ],
   "source": [
    "# Define a pool of dataset IDs\n",
    "dataset_ids = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
    "\n",
    "# Initialize the planner with total epochs, probability p1, and decay rate\n",
    "planner = LearningPathPlanner(dataset_ids, total_epochs=200, p1=0.5, decay_rate=0.3)\n",
    "\n",
    "# Generate a learning path\n",
    "learning_path = planner.generate_learning_path()\n",
    "print(\"Planned Learning Path:\", learning_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class LearningPathPlanner:\n",
    "    \"\"\"\n",
    "    A planner for generating a learning path from a pool of dataset IDs, where the probability of selecting each dataset \n",
    "    decreases exponentially with the number of times it has been viewed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_ids, total_epochs, decay_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the LearningPathPlanner.\n",
    "\n",
    "        Args:\n",
    "            dataset_ids (list): List of dataset IDs representing the pool of available datasets.\n",
    "            total_epochs (int): The total number of epochs for which to generate a learning path.\n",
    "            decay_rate (float): The rate at which revisit probability decreases exponentially with each view count.\n",
    "        \"\"\"\n",
    "        self.dataset_ids = dataset_ids  # Pool of dataset IDs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.decay_rate = decay_rate\n",
    "        self.view_count = Counter({ds_id: 0 for ds_id in dataset_ids})  # Initialize all datasets with a view count of 0\n",
    "\n",
    "    def get_exponential_probability(self, views):\n",
    "        \"\"\"\n",
    "        Calculates the probability weight for a dataset based on its view count using exponential decay.\n",
    "\n",
    "        Args:\n",
    "            views (int): The number of times the dataset has been viewed.\n",
    "\n",
    "        Returns:\n",
    "            float: The probability weight for the dataset.\n",
    "        \"\"\"\n",
    "        return math.exp(-self.decay_rate * views)\n",
    "\n",
    "    def get_next_dataset(self):\n",
    "        \"\"\"\n",
    "        Determines the next dataset ID to use based on exponential decay in probability.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the next dataset to use for training.\n",
    "        \"\"\"\n",
    "        # Calculate weights for each dataset based on exponential decay\n",
    "        weights = [self.get_exponential_probability(self.view_count[ds]) for ds in self.dataset_ids]\n",
    "        total_weight = sum(weights)\n",
    "        probabilities = [w / total_weight for w in weights]\n",
    "\n",
    "        # Randomly choose a dataset based on calculated probabilities\n",
    "        next_dataset = random.choices(self.dataset_ids, weights=probabilities, k=1)[0]\n",
    "        self.view_count[next_dataset] += 1  # Increment view count for selected dataset\n",
    "\n",
    "        return next_dataset\n",
    "\n",
    "    def generate_learning_path(self):\n",
    "        \"\"\"\n",
    "        Generates a full learning path for the specified total number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset IDs representing the planned learning path.\n",
    "        \"\"\"\n",
    "        learning_path = []\n",
    "        for _ in range(self.total_epochs):\n",
    "            next_dataset = self.get_next_dataset()\n",
    "            learning_path.append(next_dataset)\n",
    "        return learning_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned Learning Path: [6, 2, 1, 1, 2, 4, 3, 5, 4, 8, 8, 3, 4, 7, 6, 2, 8, 9, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# Define a pool of dataset IDs\n",
    "dataset_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Initialize the planner with total epochs and decay rate\n",
    "planner = LearningPathPlanner(dataset_ids, total_epochs=20, decay_rate=0.5)\n",
    "\n",
    "# Generate a learning path\n",
    "learning_path = planner.generate_learning_path()\n",
    "print(\"Planned Learning Path:\", learning_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNetwork, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Reduces dimensions by half\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Reduces dimensions by another half\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to ensure a consistent, small output size regardless of input\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=(4, 4))  # Downsample to 4x4 spatial size\n",
    "\n",
    "        # Fully connected layers with a smaller input size\n",
    "        self.lin_1 = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 64),  # Reduced input size\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lin = nn.Linear(in_features=64, out_features=38)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, a=0.1)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, a=0.1)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.ap(x)  # Adaptive pooling to 4x4\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.lin_1(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "    def predict_on_output(self, output): \n",
    "        output = nn.Softmax(dim=1)(output)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model = SmallNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SmallNetwork                             [129, 38]                 --\n",
       "├─Sequential: 1-1                        [129, 32, 32, 31]         --\n",
       "│    └─Conv2d: 2-1                       [129, 16, 128, 126]       160\n",
       "│    └─BatchNorm2d: 2-2                  [129, 16, 128, 126]       32\n",
       "│    └─ReLU: 2-3                         [129, 16, 128, 126]       --\n",
       "│    └─MaxPool2d: 2-4                    [129, 16, 64, 63]         --\n",
       "│    └─Conv2d: 2-5                       [129, 32, 64, 63]         4,640\n",
       "│    └─BatchNorm2d: 2-6                  [129, 32, 64, 63]         64\n",
       "│    └─ReLU: 2-7                         [129, 32, 64, 63]         --\n",
       "│    └─MaxPool2d: 2-8                    [129, 32, 32, 31]         --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [129, 32, 4, 4]           --\n",
       "├─Sequential: 1-3                        [129, 64]                 --\n",
       "│    └─Linear: 2-9                       [129, 64]                 32,832\n",
       "│    └─Dropout: 2-10                     [129, 64]                 --\n",
       "│    └─BatchNorm1d: 2-11                 [129, 64]                 128\n",
       "│    └─ReLU: 2-12                        [129, 64]                 --\n",
       "├─Linear: 1-4                            [129, 38]                 2,470\n",
       "==========================================================================================\n",
       "Total params: 40,326\n",
       "Trainable params: 40,326\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.75\n",
       "==========================================================================================\n",
       "Input size (MB): 8.32\n",
       "Forward/backward pass size (MB): 799.09\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 807.57\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(129, 1, 128, 126))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultitaskingAdapter(nn.Module):\n",
    "    def __init__(self, task_adapters):\n",
    "        \"\"\"\n",
    "        A plug-in module for deriving task-specific representations.\n",
    "        \n",
    "        Args:\n",
    "            task_adapters (list of nn.Module): A list of task-specific layers.\n",
    "        \"\"\"\n",
    "        super(MultitaskingAdapter, self).__init__()\n",
    "        self.task_adapters = nn.ModuleList(task_adapters)\n",
    "\n",
    "    def forward(self, shared_representation, task_id):\n",
    "        \"\"\"\n",
    "        Forward pass through the task-specific adapter.\n",
    "        \n",
    "        Args:\n",
    "            shared_representation (torch.Tensor): Input feature from the shared trunk.\n",
    "            task_id (int): Index of the task-specific adapter to use.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Task-specific output.\n",
    "        \"\"\"\n",
    "        return self.task_adapters[task_id](shared_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_dim, shared_dim, task_adapters):\n",
    "        \"\"\"\n",
    "        A multi-task learning model with a shared trunk and task adapters.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension.\n",
    "            shared_dim (int): Output feature dimension of the shared trunk.\n",
    "            task_adapters (list of nn.Module): Task-specific adapters.\n",
    "        \"\"\"\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Shared trunk (feature extractor)\n",
    "        self.shared_trunk = nn.Sequential(\n",
    "            nn.Linear(input_dim, shared_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(shared_dim, shared_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Multitasking adapter module\n",
    "        self.task_adapter = MultitaskingAdapter(task_adapters)\n",
    "\n",
    "    def forward(self, x, task_id):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            task_id (int): Task identifier.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Task-specific output.\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_trunk(x)\n",
    "        task_output = self.task_adapter(shared_features, task_id)\n",
    "        return task_output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = 64\n",
    "    shared_dim = 16\n",
    "\n",
    "    # Define task-specific layers\n",
    "    task_adapters = [\n",
    "        nn.Linear(shared_dim, 10),  # Task 1: Classification (10 classes)\n",
    "        nn.Linear(shared_dim, 1),   # Task 2: Regression\n",
    "        nn.Sequential(nn.Linear(shared_dim, 8), nn.ReLU(), nn.Linear(8, 2))  # Task 3: Another classification task\n",
    "    ]\n",
    "\n",
    "    # Instantiate the multi-task model\n",
    "    model = MultiTaskModel(input_dim, shared_dim, task_adapters)\n",
    "\n",
    "    # Dummy input\n",
    "    x = torch.randn(5, input_dim)  # Batch size 5, input dimension 64\n",
    "    task_id = 1  # Select task-specific adapter (Regression)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x, task_id)\n",
    "    print(output.shape)  # Should match the output dimension of task_adapters[task_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for Task Adapter 0:\n",
      "tensor([[ 1.7546e-02,  2.4621e-02,  2.2493e-03, -3.5681e-03,  3.7540e-03,\n",
      "          7.3206e-03,  5.4696e-03,  0.0000e+00,  3.2171e-04,  0.0000e+00,\n",
      "         -1.8941e-03,  1.5458e-02, -1.1216e-02,  0.0000e+00, -5.4098e-04,\n",
      "          0.0000e+00],\n",
      "        [ 9.0469e-03,  4.3736e-04,  5.9761e-03,  5.1451e-03,  6.8116e-03,\n",
      "          2.0615e-03, -1.0413e-02,  0.0000e+00,  6.6588e-04,  0.0000e+00,\n",
      "         -7.0767e-04, -9.2252e-03, -1.6431e-02,  0.0000e+00,  7.6723e-04,\n",
      "          0.0000e+00],\n",
      "        [ 2.5527e-02,  1.2481e-02,  1.4572e-02, -8.0933e-03,  1.0621e-02,\n",
      "          9.0402e-03,  4.1339e-02,  0.0000e+00,  5.5098e-04,  0.0000e+00,\n",
      "          1.5218e-03,  2.2084e-02,  3.9293e-02,  0.0000e+00, -1.4122e-03,\n",
      "          0.0000e+00],\n",
      "        [-4.4750e-02, -3.0772e-02,  8.5538e-03, -1.1916e-02, -1.8562e-02,\n",
      "         -2.1351e-02, -5.3711e-02,  0.0000e+00,  7.8703e-04,  0.0000e+00,\n",
      "         -3.7322e-03, -2.6648e-02, -3.3885e-02,  0.0000e+00, -1.7989e-03,\n",
      "          0.0000e+00],\n",
      "        [ 4.3595e-03,  1.8925e-03, -8.1002e-04, -1.0497e-02,  1.4440e-02,\n",
      "          1.2202e-02, -6.4994e-03,  0.0000e+00,  1.3468e-04,  0.0000e+00,\n",
      "         -3.7331e-03,  5.0070e-04, -1.8148e-02,  0.0000e+00, -1.7496e-03,\n",
      "          0.0000e+00],\n",
      "        [ 1.8389e-02,  2.0419e-02,  1.6247e-02, -7.7835e-03, -1.0123e-03,\n",
      "          7.7034e-04,  1.8991e-02,  0.0000e+00,  9.4710e-04,  0.0000e+00,\n",
      "         -2.3331e-04,  2.0110e-02,  1.3614e-02,  0.0000e+00, -1.2401e-03,\n",
      "          0.0000e+00],\n",
      "        [-1.1584e-02, -9.1468e-03, -1.2401e-02, -1.4776e-03, -3.9863e-03,\n",
      "         -1.7349e-04,  1.8739e-02,  0.0000e+00, -1.2482e-03,  0.0000e+00,\n",
      "          2.3625e-03,  6.1517e-03,  2.9694e-02,  0.0000e+00, -2.2066e-04,\n",
      "          0.0000e+00],\n",
      "        [ 7.7622e-04, -2.3014e-02, -9.3939e-03, -9.8615e-04,  2.4383e-02,\n",
      "          1.6092e-02,  7.6346e-03,  0.0000e+00, -7.1770e-04,  0.0000e+00,\n",
      "         -1.1162e-04, -1.3380e-02,  8.2396e-03,  0.0000e+00, -3.6817e-04,\n",
      "          0.0000e+00],\n",
      "        [ 2.5001e-04,  7.1329e-03,  4.8964e-03, -1.2882e-03, -4.4650e-03,\n",
      "         -3.4837e-03, -1.2201e-02,  0.0000e+00,  5.2058e-04,  0.0000e+00,\n",
      "         -1.4556e-03,  1.0489e-04, -1.6329e-02,  0.0000e+00, -1.5334e-04,\n",
      "          0.0000e+00],\n",
      "        [-4.2699e-02, -1.6901e-02, -1.5063e-02, -1.4901e-03, -2.8233e-02,\n",
      "         -1.9723e-02, -1.9813e-02,  0.0000e+00, -1.1918e-03,  0.0000e+00,\n",
      "          8.8031e-04, -6.1448e-03,  1.0802e-03,  0.0000e+00, -2.2420e-05,\n",
      "          0.0000e+00]])\n",
      "tensor([ 0.0143,  0.0057,  0.1102, -0.1597, -0.0377,  0.0571,  0.0115,  0.0062,\n",
      "        -0.0238, -0.0991])\n",
      "Gradients for Task Adapter 1:\n",
      "None\n",
      "None\n",
      "Gradients for Task Adapter 2:\n",
      "None\n",
      "None\n",
      "✅ Gradient flow test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_gradient_flow():\n",
    "    # Define input dimensions\n",
    "    input_dim = 64\n",
    "    shared_dim = 16\n",
    "\n",
    "    # Define task-specific layers\n",
    "    task_adapters = [\n",
    "        nn.Linear(shared_dim, 10),  # Task 1: Classification (10 classes)\n",
    "        nn.Linear(shared_dim, 1),   # Task 2: Regression\n",
    "        nn.Linear(shared_dim, 8)    # Task 3: Another transformation\n",
    "    ]\n",
    "\n",
    "    # Instantiate the multi-task model\n",
    "    model = MultiTaskModel(input_dim, shared_dim, task_adapters)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Select task ID to test (e.g., task 1)\n",
    "    task_id = 0\n",
    "\n",
    "    # Generate dummy input and target\n",
    "    x = torch.randn(5, input_dim)  # Batch size 5\n",
    "    target = torch.randn(5, 10)  # Task 1 output dimension\n",
    "\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x, task_id)\n",
    "\n",
    "    # Compute loss (MSE for simplicity)\n",
    "    loss = nn.MSELoss()(output, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradients of task adapters\n",
    "    for i, adapter in enumerate(model.task_adapter.task_adapters):\n",
    "        print(f\"Gradients for Task Adapter {i}:\")\n",
    "        for p in adapter.parameters():\n",
    "            print(p.grad)  # Should be None for non-selected adapters\n",
    "\n",
    "    # Check gradient flow in shared trunk\n",
    "    shared_grads = [p.grad for p in model.shared_trunk.parameters() if p.grad is not None]\n",
    "    assert all(g is not None for g in shared_grads), \"Gradient did not flow through shared trunk\"\n",
    "\n",
    "    # Check gradient flow in selected task adapter\n",
    "    task_grads = [p.grad for p in model.task_adapter.task_adapters[task_id].parameters() if p.grad is not None]\n",
    "    assert all(g is not None for g in task_grads), \"Gradient did not flow through selected task adapter\"\n",
    "\n",
    "    # Ensure non-selected task adapters do NOT get gradients\n",
    "    for i, adapter in enumerate(model.task_adapter.task_adapters):\n",
    "        if i != task_id:\n",
    "            adapter_grads = [p.grad for p in adapter.parameters()]\n",
    "            assert all(g is None for g in adapter_grads), f\"Unexpected gradient in task adapter {i}\"\n",
    "\n",
    "    print(\"✅ Gradient flow test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_gradient_flow()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pros22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
