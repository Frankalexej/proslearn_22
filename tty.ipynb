{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from librosa.util import index_to_slice\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# meta data file (with syllable information)\n",
    "from PretermDataLoader import DataLoader as dl\n",
    "import audioPreprocess as ap\n",
    "import random\n",
    "import dataPreprocess as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafile = 'guide_test_syllableInfor.pkl'\n",
    "datanum = 1000 # assume that you want 10000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dl()\n",
    "meta = loader.get_metadata(metafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mels,indexes = loader.load_data('mel',datanum)\n",
    "spectrograms = loader.load_data('spectrogram',datanum,indexes)[0]\n",
    "soundpaths = [meta['filepath'][i] for i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 126)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mels: [datanum, (mel_dim, length)]\n",
    "# 長度固定，因此可以直接轉成numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franklhtan/anaconda3/envs/pros22/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# or you can get a bunch of indexlists that keeps the original distribution first\n",
    "meta['index'] = meta.index\n",
    "indexset = meta[['stress_type','index','suid']]\n",
    "# Split into subsets\n",
    "subsets_index = dp.split_into_subsets(indexset, num_subsets=100, sortkey='stress_type')\n",
    "# #Display the resulting subsets\n",
    "# for i, subset in enumerate(subsets):\n",
    "#     dp.checkDistribution(subset['stress_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stress_type</th>\n",
       "      <th>index</th>\n",
       "      <th>suid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98594</th>\n",
       "      <td>1</td>\n",
       "      <td>98594</td>\n",
       "      <td>446-123502-0022-0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22107</th>\n",
       "      <td>0</td>\n",
       "      <td>22107</td>\n",
       "      <td>1502-122619-0078-0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27266</th>\n",
       "      <td>1</td>\n",
       "      <td>27266</td>\n",
       "      <td>2092-145706-0037-0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69203</th>\n",
       "      <td>0</td>\n",
       "      <td>69203</td>\n",
       "      <td>3240-131232-0006-0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98418</th>\n",
       "      <td>0</td>\n",
       "      <td>98418</td>\n",
       "      <td>446-123502-0017-0045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109917</th>\n",
       "      <td>1</td>\n",
       "      <td>109917</td>\n",
       "      <td>5163-39921-0022-0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125275</th>\n",
       "      <td>1</td>\n",
       "      <td>125275</td>\n",
       "      <td>78-368-0045-0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120438</th>\n",
       "      <td>1</td>\n",
       "      <td>120438</td>\n",
       "      <td>7402-59171-0047-0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80574</th>\n",
       "      <td>0</td>\n",
       "      <td>80574</td>\n",
       "      <td>3807-4955-0009-0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20314</th>\n",
       "      <td>1</td>\n",
       "      <td>20314</td>\n",
       "      <td>1502-122619-0033-0033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1392 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stress_type   index                   suid\n",
       "98594            1   98594   446-123502-0022-0002\n",
       "22107            0   22107  1502-122619-0078-0047\n",
       "27266            1   27266  2092-145706-0037-0037\n",
       "69203            0   69203  3240-131232-0006-0042\n",
       "98418            0   98418   446-123502-0017-0045\n",
       "...            ...     ...                    ...\n",
       "109917           1  109917   5163-39921-0022-0007\n",
       "125275           1  125275       78-368-0045-0016\n",
       "120438           1  120438   7402-59171-0047-0042\n",
       "80574            0   80574    3807-4955-0009-0038\n",
       "20314            1   20314  1502-122619-0033-0033\n",
       "\n",
       "[1392 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets_index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it seems certain that the loading is not on-the-full. \n",
    "Therefore, we can happily design loading functions based on this mechanism. We don't have to design a \"load-for-all\" mechanism, because, anyway, the time consumption and memory usage is not \"the full\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "Design how we pass in data for different dataloaders and how we plan learning curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be easy to do just following our old practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, Counter\n",
    "\n",
    "class LearningPathPlanner:\n",
    "    \"\"\"\n",
    "    A planner for generating a learning path from a pool of dataset IDs, allowing for control over \n",
    "    probability of selecting new or old datasets, and imposing revisit limits on old datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_ids, total_epochs, p1=0.5, revisit_limit=5):\n",
    "        \"\"\"\n",
    "        Initializes the LearningPathPlanner.\n",
    "\n",
    "        Args:\n",
    "            dataset_ids (list): List of dataset IDs representing the pool of available datasets.\n",
    "            total_epochs (int): The total number of epochs for which to generate a learning path.\n",
    "            p1 (float): Probability of selecting a new dataset in each epoch (between 0 and 1).\n",
    "            revisit_limit (int): Maximum number of times each dataset can be revisited before being removed.\n",
    "        \"\"\"\n",
    "        self.dataset_ids = dataset_ids  # Pool of dataset IDs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.p1 = p1\n",
    "        self.revisit_limit = revisit_limit\n",
    "        self.new_datasets = set(dataset_ids)  # Datasets not yet seen\n",
    "        self.old_datasets = deque()  # Queue to track datasets that have been used\n",
    "        self.revisit_count = Counter()  # Counter to track the number of revisits for each dataset\n",
    "\n",
    "    def get_next_dataset(self):\n",
    "        \"\"\"\n",
    "        Determines the next dataset ID to use based on the learning path logic.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the next dataset to use for training.\n",
    "        \"\"\"\n",
    "        # Decide whether to select a new or an old dataset based on probability p1\n",
    "        if self.new_datasets and random.random() < self.p1:\n",
    "            # Choose a new dataset\n",
    "            next_dataset = self.new_datasets.pop()\n",
    "            self.old_datasets.append(next_dataset)  # Move to old datasets\n",
    "            self.revisit_count[next_dataset] = 0  # Initialize revisit count\n",
    "\n",
    "        else:\n",
    "            # Choose an old dataset, if any are available\n",
    "            if self.old_datasets:\n",
    "                next_dataset = random.choice(self.old_datasets)\n",
    "                self.revisit_count[next_dataset] += 1\n",
    "\n",
    "                # Check if the revisit limit is reached\n",
    "                if self.revisit_count[next_dataset] >= self.revisit_limit:\n",
    "                    # Remove dataset if revisit limit is reached\n",
    "                    self.old_datasets.remove(next_dataset)\n",
    "                    del self.revisit_count[next_dataset]\n",
    "            else:\n",
    "                # Fallback to a new dataset if no old datasets are available\n",
    "                next_dataset = self.new_datasets.pop()\n",
    "                self.old_datasets.append(next_dataset)\n",
    "                self.revisit_count[next_dataset] = 0\n",
    "\n",
    "        return next_dataset\n",
    "\n",
    "    def generate_learning_path(self):\n",
    "        \"\"\"\n",
    "        Generates a full learning path for the specified total number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset IDs representing the planned learning path.\n",
    "        \"\"\"\n",
    "        learning_path = []\n",
    "        for _ in range(self.total_epochs):\n",
    "            next_dataset = self.get_next_dataset()\n",
    "            learning_path.append(next_dataset)\n",
    "        return learning_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned Learning Path: [1, 1, 2, 3, 1, 4, 1, 5, 6, 7, 8, 4, 9, 6, 3, 2, 5, 9, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "# Define a pool of dataset IDs\n",
    "dataset_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Initialize the planner with total epochs, probability p1, and revisit limit\n",
    "planner = LearningPathPlanner(dataset_ids, total_epochs=20, p1=0.5, revisit_limit=3)\n",
    "\n",
    "# Generate a learning path\n",
    "learning_path = planner.generate_learning_path()\n",
    "print(\"Planned Learning Path:\", learning_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import deque, Counter\n",
    "\n",
    "class LearningPathPlanner:\n",
    "    \"\"\"\n",
    "    A planner for generating a learning path from a pool of dataset IDs, where revisit probability exponentially decreases \n",
    "    with each visit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_ids, total_epochs, p1=0.5, decay_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the LearningPathPlanner.\n",
    "\n",
    "        Args:\n",
    "            dataset_ids (list): List of dataset IDs representing the pool of available datasets.\n",
    "            total_epochs (int): The total number of epochs for which to generate a learning path.\n",
    "            p1 (float): Probability of selecting a new dataset in each epoch (between 0 and 1).\n",
    "            decay_rate (float): The rate at which revisit probability decreases exponentially with each visit.\n",
    "        \"\"\"\n",
    "        self.dataset_ids = dataset_ids  # Pool of dataset IDs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.p1 = p1\n",
    "        self.decay_rate = decay_rate\n",
    "        self.new_datasets = set(dataset_ids)  # Datasets not yet seen\n",
    "        self.old_datasets = deque()  # Queue to track datasets that have been used\n",
    "        self.visit_count = Counter()  # Counter to track the number of visits for each dataset\n",
    "\n",
    "    def get_exponential_probability(self, visits):\n",
    "        \"\"\"\n",
    "        Calculates the probability weight for a dataset based on its visit count using exponential decay.\n",
    "\n",
    "        Args:\n",
    "            visits (int): The number of times the dataset has been visited.\n",
    "\n",
    "        Returns:\n",
    "            float: The probability weight for the dataset.\n",
    "        \"\"\"\n",
    "        return math.exp(-self.decay_rate * visits)\n",
    "\n",
    "    def get_next_dataset(self):\n",
    "        \"\"\"\n",
    "        Determines the next dataset ID to use based on the learning path logic, with exponential decay in revisit probability.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the next dataset to use for training.\n",
    "        \"\"\"\n",
    "        # Decide whether to select a new or an old dataset based on probability p1\n",
    "        if self.new_datasets and random.random() < self.p1:\n",
    "            # Choose a new dataset\n",
    "            next_dataset = self.new_datasets.pop()\n",
    "            self.old_datasets.append(next_dataset)  # Move to old datasets\n",
    "            self.visit_count[next_dataset] = 0  # Initialize visit count\n",
    "\n",
    "        else:\n",
    "            # Choose an old dataset with probability exponentially decreasing by visit count\n",
    "            if self.old_datasets:\n",
    "                # Calculate weights for old datasets based on exponential decay\n",
    "                weights = [self.get_exponential_probability(self.visit_count[ds]) for ds in self.old_datasets]\n",
    "                total_weight = sum(weights)\n",
    "                probabilities = [w / total_weight for w in weights]\n",
    "\n",
    "                # Randomly choose an old dataset based on calculated probabilities\n",
    "                next_dataset = random.choices(list(self.old_datasets), weights=probabilities, k=1)[0]\n",
    "                self.visit_count[next_dataset] += 1  # Increment visit count\n",
    "\n",
    "            else:\n",
    "                # Fallback to a new dataset if no old datasets are available\n",
    "                next_dataset = self.new_datasets.pop()\n",
    "                self.old_datasets.append(next_dataset)\n",
    "                self.visit_count[next_dataset] = 0\n",
    "\n",
    "        return next_dataset\n",
    "\n",
    "    def generate_learning_path(self):\n",
    "        \"\"\"\n",
    "        Generates a full learning path for the specified total number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset IDs representing the planned learning path.\n",
    "        \"\"\"\n",
    "        learning_path = []\n",
    "        for _ in range(self.total_epochs):\n",
    "            next_dataset = self.get_next_dataset()\n",
    "            learning_path.append(next_dataset)\n",
    "        return learning_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned Learning Path: ['D', 'D', 'D', 'F', 'E', 'C', 'B', 'I', 'J', 'J', 'E', 'K', 'G', 'H', 'H', 'B', 'I', 'K', 'A', 'K', 'A', 'G', 'A', 'C', 'J', 'H', 'I', 'H', 'I', 'E', 'G', 'C', 'G', 'D', 'F', 'C', 'K', 'H', 'B', 'C', 'I', 'K', 'B', 'G', 'D', 'A', 'D', 'C', 'H', 'J', 'E', 'A', 'K', 'E', 'G', 'J', 'K', 'C', 'E', 'F', 'F', 'D', 'E', 'D', 'I', 'B', 'B', 'F', 'D', 'G', 'J', 'A', 'E', 'K', 'F', 'B', 'E', 'F', 'A', 'J', 'B', 'J', 'A', 'J', 'F', 'I', 'D', 'H', 'H', 'G', 'C', 'F', 'B', 'H', 'B', 'B', 'I', 'D', 'K', 'C', 'D', 'E', 'J', 'H', 'A', 'F', 'E', 'I', 'A', 'K', 'K', 'G', 'I', 'F', 'B', 'F', 'F', 'A', 'J', 'H', 'C', 'J', 'H', 'C', 'B', 'H', 'I', 'I', 'B', 'J', 'G', 'G', 'D', 'I', 'H', 'H', 'K', 'B', 'C', 'A', 'H', 'G', 'D', 'D', 'H', 'K', 'E', 'E', 'G', 'F', 'A', 'C', 'F', 'E', 'G', 'I', 'G', 'A', 'F', 'E', 'C', 'K', 'A', 'A', 'J', 'B', 'B', 'J', 'E', 'K', 'K', 'D', 'A', 'A', 'J', 'E', 'H', 'C', 'B', 'A', 'I', 'E', 'I', 'K', 'F', 'E', 'H', 'E', 'C', 'B', 'K', 'C', 'A', 'G', 'J', 'F', 'J', 'I', 'I', 'F']\n"
     ]
    }
   ],
   "source": [
    "# Define a pool of dataset IDs\n",
    "dataset_ids = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
    "\n",
    "# Initialize the planner with total epochs, probability p1, and decay rate\n",
    "planner = LearningPathPlanner(dataset_ids, total_epochs=200, p1=0.5, decay_rate=0.3)\n",
    "\n",
    "# Generate a learning path\n",
    "learning_path = planner.generate_learning_path()\n",
    "print(\"Planned Learning Path:\", learning_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class LearningPathPlanner:\n",
    "    \"\"\"\n",
    "    A planner for generating a learning path from a pool of dataset IDs, where the probability of selecting each dataset \n",
    "    decreases exponentially with the number of times it has been viewed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_ids, total_epochs, decay_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the LearningPathPlanner.\n",
    "\n",
    "        Args:\n",
    "            dataset_ids (list): List of dataset IDs representing the pool of available datasets.\n",
    "            total_epochs (int): The total number of epochs for which to generate a learning path.\n",
    "            decay_rate (float): The rate at which revisit probability decreases exponentially with each view count.\n",
    "        \"\"\"\n",
    "        self.dataset_ids = dataset_ids  # Pool of dataset IDs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.decay_rate = decay_rate\n",
    "        self.view_count = Counter({ds_id: 0 for ds_id in dataset_ids})  # Initialize all datasets with a view count of 0\n",
    "\n",
    "    def get_exponential_probability(self, views):\n",
    "        \"\"\"\n",
    "        Calculates the probability weight for a dataset based on its view count using exponential decay.\n",
    "\n",
    "        Args:\n",
    "            views (int): The number of times the dataset has been viewed.\n",
    "\n",
    "        Returns:\n",
    "            float: The probability weight for the dataset.\n",
    "        \"\"\"\n",
    "        return math.exp(-self.decay_rate * views)\n",
    "\n",
    "    def get_next_dataset(self):\n",
    "        \"\"\"\n",
    "        Determines the next dataset ID to use based on exponential decay in probability.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the next dataset to use for training.\n",
    "        \"\"\"\n",
    "        # Calculate weights for each dataset based on exponential decay\n",
    "        weights = [self.get_exponential_probability(self.view_count[ds]) for ds in self.dataset_ids]\n",
    "        total_weight = sum(weights)\n",
    "        probabilities = [w / total_weight for w in weights]\n",
    "\n",
    "        # Randomly choose a dataset based on calculated probabilities\n",
    "        next_dataset = random.choices(self.dataset_ids, weights=probabilities, k=1)[0]\n",
    "        self.view_count[next_dataset] += 1  # Increment view count for selected dataset\n",
    "\n",
    "        return next_dataset\n",
    "\n",
    "    def generate_learning_path(self):\n",
    "        \"\"\"\n",
    "        Generates a full learning path for the specified total number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset IDs representing the planned learning path.\n",
    "        \"\"\"\n",
    "        learning_path = []\n",
    "        for _ in range(self.total_epochs):\n",
    "            next_dataset = self.get_next_dataset()\n",
    "            learning_path.append(next_dataset)\n",
    "        return learning_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned Learning Path: [6, 2, 1, 1, 2, 4, 3, 5, 4, 8, 8, 3, 4, 7, 6, 2, 8, 9, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# Define a pool of dataset IDs\n",
    "dataset_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Initialize the planner with total epochs and decay rate\n",
    "planner = LearningPathPlanner(dataset_ids, total_epochs=20, decay_rate=0.5)\n",
    "\n",
    "# Generate a learning path\n",
    "learning_path = planner.generate_learning_path()\n",
    "print(\"Planned Learning Path:\", learning_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNetwork, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Reduces dimensions by half\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Reduces dimensions by another half\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to ensure a consistent, small output size regardless of input\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=(4, 4))  # Downsample to 4x4 spatial size\n",
    "\n",
    "        # Fully connected layers with a smaller input size\n",
    "        self.lin_1 = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 64),  # Reduced input size\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lin = nn.Linear(in_features=64, out_features=38)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, a=0.1)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, a=0.1)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.ap(x)  # Adaptive pooling to 4x4\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.lin_1(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "    def predict_on_output(self, output): \n",
    "        output = nn.Softmax(dim=1)(output)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model = SmallNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SmallNetwork                             [129, 38]                 --\n",
       "├─Sequential: 1-1                        [129, 32, 32, 31]         --\n",
       "│    └─Conv2d: 2-1                       [129, 16, 128, 126]       160\n",
       "│    └─BatchNorm2d: 2-2                  [129, 16, 128, 126]       32\n",
       "│    └─ReLU: 2-3                         [129, 16, 128, 126]       --\n",
       "│    └─MaxPool2d: 2-4                    [129, 16, 64, 63]         --\n",
       "│    └─Conv2d: 2-5                       [129, 32, 64, 63]         4,640\n",
       "│    └─BatchNorm2d: 2-6                  [129, 32, 64, 63]         64\n",
       "│    └─ReLU: 2-7                         [129, 32, 64, 63]         --\n",
       "│    └─MaxPool2d: 2-8                    [129, 32, 32, 31]         --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [129, 32, 4, 4]           --\n",
       "├─Sequential: 1-3                        [129, 64]                 --\n",
       "│    └─Linear: 2-9                       [129, 64]                 32,832\n",
       "│    └─Dropout: 2-10                     [129, 64]                 --\n",
       "│    └─BatchNorm1d: 2-11                 [129, 64]                 128\n",
       "│    └─ReLU: 2-12                        [129, 64]                 --\n",
       "├─Linear: 1-4                            [129, 38]                 2,470\n",
       "==========================================================================================\n",
       "Total params: 40,326\n",
       "Trainable params: 40,326\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.75\n",
       "==========================================================================================\n",
       "Input size (MB): 8.32\n",
       "Forward/backward pass size (MB): 799.09\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 807.57\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(129, 1, 128, 126))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pros22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
